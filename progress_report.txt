This document records the progress of the adversarial DNN project.
The document is intended for me and colleagues to have a reference of things I have tried throughout the project.

10.7.2017

- built basic mnist classifier with 2 hidden layers and one softmax layer
- Tuned parameters:
    - learning rate
    - hidden unit numbers
    - mini-batch size
    - number of epochs
- Not tuned parameters:
    For Adam Optimizer:
        beta1=0.9
        beta2=0.999
        epsilon=1e-08
- Running parameter tuning overnight, will record optimal hyperparameters tomorrow and results tomorrow.

10.8.2017
- Collect result
    Optimal batch size is:100 
    Optimal number of neurons for layer 1 is:200 (25, 50, 100, 200, 300)
    Optimal number of neurons for layer 2 is:300 (25, 50, 100, 200, 300)
    Optimal number of neurons for layer 3 is:10
    Optimal epoch number is:8 (1 - 30)
    Best validation accuracy:0.9858
    Optimal train accuracy:0.99
    Test accuracy is:0.9795

10.11.2017
- Be able to save and restored trained mnist model
- Develope a basic FGSM attacker on the mnist classifier and demonstrate what a perturbed image looks like and show that the mnist classifier misclassifies on a test image
- Draw the line graph illustrating how changes in epsilon may lead to drastic decrease in the algorithm's performace on the test set.

10.20
- Reproduced GoodFellow's claim in his EXPLAINING AND HARNESSING ADVERSARIAL EXAMPLES paper
"We found that training with an adversarial objective function based on the fast gradient sign method
was an effective regularizer:
J˜(θ, x, y) = αJ(θ, x, y) + (1 − α)J(θ, x + epsilon*sign (∇xJ(θ, x, y))""
- vulnerability_score_defense_mnist.py (Classifier trained with adversarial training)
New accuracy with old hyperparameters used in the original mnist becomes: 0.9859
- mnist_fgsm_attack_with_vulterm.py (Attacking the classifier above)

We drew the accuracy vs epsilon curve for the new effect of FGSM and show that the new classifier with adversarrial training is a lot more resistant to this attack. The curve is shown in fgsm_adversarial_training_updated_loss_attack_new_loss and fgsm_adversarial_training_updated_loss_attack_original_loss. The diference is when the FGSM is trained with the original loss function versus the new loss function with the vulnerability term.
