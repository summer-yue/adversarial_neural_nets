This document records the progress of the adversarial DNN project.
The document is intended for me and colleagues to have a reference of things I have tried throughout the project.

10.7.2017

- built basic mnist classifier with 2 hidden layers and one softmax layer
- Tuned parameters:
    - learning rate
    - hidden unit numbers
    - mini-batch size
    - number of epochs
- Not tuned parameters:
    For Adam Optimizer:
        beta1=0.9
        beta2=0.999
        epsilon=1e-08
- Running parameter tuning overnight, will record optimal hyperparameters tomorrow and results tomorrow.

10.8.2017
- Collect result
    Optimal batch size is:100 
    Optimal number of neurons for layer 1 is:200 (25, 50, 100, 200, 300)
    Optimal number of neurons for layer 2 is:300 (25, 50, 100, 200, 300)
    Optimal number of neurons for layer 3 is:10
    Optimal epoch number is:8 (1 - 30)
    Best validation accuracy:0.9858
    Optimal train accuracy:0.99
    Test accuracy is:0.9795

10.11.2017
- Be able to save and restored trained mnist model
- Develope a basic FGSM attacker on the mnist classifier and demonstrate what a perturbed image looks like and show that the mnist classifier misclassifies on a test image
- Draw the line graph illustrating how changes in epsilon may lead to drastic decrease in the algorithm's performace on the test set.

10.20
- Reproduced GoodFellow's claim in his EXPLAINING AND HARNESSING ADVERSARIAL EXAMPLES paper
"We found that training with an adversarial objective function based on the fast gradient sign method
was an effective regularizer:
J˜(θ, x, y) = αJ(θ, x, y) + (1 − α)J(θ, x + epsilon*sign (∇xJ(θ, x, y))""
- vulnerability_score_defense_mnist.py (Classifier trained with adversarial training)
New accuracy with old hyperparameters used in the original mnist becomes: 0.9859
- mnist_fgsm_attack_with_vulterm.py (Attacking the classifier above)

We drew the accuracy vs epsilon curve for the new effect of FGSM and show that the new classifier with adversarrial training is a lot more resistant to this attack. The curve is shown in fgsm_adversarial_training_updated_loss_attack_new_loss and fgsm_adversarial_training_updated_loss_attack_original_loss. The diference is when the FGSM is trained with the original loss function versus the new loss function with the vulnerability term.

10.23
Run different epochs and observe the error rate
For epoch:5
Train accuracy is:0.989545
Test accuracy is:0.9764
For epoch:6
Train accuracy is:0.994745
Test accuracy is:0.9765
For epoch:7
Train accuracy is:0.997727
Test accuracy is:0.9798
For epoch:8
Train accuracy is:0.996727
Test accuracy is:0.9774
For epoch:9
Train accuracy is:0.998364
Test accuracy is:0.9792
For epoch:10
Train accuracy is:0.999891
Test accuracy is:0.9844
For epoch:11
Train accuracy is:0.999473
Test accuracy is:0.982
For epoch:12
Train accuracy is:0.999036
Test accuracy is:0.9827
For epoch:13
Train accuracy is:0.999709
Test accuracy is:0.9829
For epoch:14
Train accuracy is:1.0
Test accuracy is:0.9848

Results in results/structures_vs_robustness of epoch_number vs accuracy seems counterintuitive.
If overfitting makes adversarial examples more likely,
increase in epoch number should cause the accuracy after a fixed attack to go down, as it's overfitted. 
The opposite result is observed. The same happens when we increase the neuron numbers.

Then we applied l2 regularization, and then noticed that it becomes more resistant to attacks. 
This is indeed expected.