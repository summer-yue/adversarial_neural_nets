This document records the progress of the adversarial DNN project.
The document is intended for me and colleagues to have a reference of things I have tried throughout the project.

10.7.2017

- built basic mnist classifier with 2 hidden layers and one softmax layer
- Tuned parameters:
    - learning rate
    - hidden unit numbers
    - mini-batch size
    - number of epochs
- Not tuned parameters:
    For Adam Optimizer:
        beta1=0.9
        beta2=0.999
        epsilon=1e-08
- Running parameter tuning overnight, will record optimal hyperparameters tomorrow and results tomorrow.

10.8.2017
- Collect result
    Optimal batch size is:100 
    Optimal number of neurons for layer 1 is:200 (25, 50, 100, 200, 300)
    Optimal number of neurons for layer 2 is:300 (25, 50, 100, 200, 300)
    Optimal number of neurons for layer 3 is:10
    Optimal epoch number is:8 (1 - 30)
    Best validation accuracy:0.9858
    Optimal train accuracy:0.99
    Test accuracy is:0.9795

10.11.2017
- Be able to save and restored trained mnist model
- Develope a basic FGSM attacker on the mnist classifier and demonstrate what a perturbed image looks like and show that the mnist classifier misclassifies on a test image